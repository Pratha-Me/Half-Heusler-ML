{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\nWhere did data come from?\n\nThe material id needs to be prepended by \"mp-\". The job is performed below","metadata":{"id":"76e9a234-e8d2-4c06-bc62-3fcfaba7b642"}},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"id":"zwdUed_ccjQt","outputId":"8d9ae26e-79fe-4405-a901-a37c52f04292"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# import sys\n# module_path = os.path.abspath(os.path.join(''))\n# if module_path not in sys.path:\n#     sys.path.append(module_path)\n    \n# print (module_path)\n\n# from utils.editdf import EditFile \n\n# ed = EditFile()\n# ed.generateFile('assets/HalfHeusler.csv', 'assets/Heusler compound.csv')\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')\n\n# %pip install pymatgen\n# %pip install sklearn\n# %pip install --upgrade tensorflow\n# %pip install keras\n\nimport os.path\nimport pandas as pd\n\n# import pymatgen.core as pg\n# from pymatgen.ext.matproj import MPRester\nfrom math import sqrt\nimport sys\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nimport tensorflow as tf\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n# dirPath= 'drive/Mydrive/'\n# dirPath= 'assets/'\ndirPath= '../input/assets/'","metadata":{"id":"1ff8be58-9ef0-4180-97fe-af8210c01cff","execution":{"iopub.status.busy":"2021-08-23T13:13:40.028022Z","iopub.execute_input":"2021-08-23T13:13:40.028356Z","iopub.status.idle":"2021-08-23T13:13:40.037809Z","shell.execute_reply.started":"2021-08-23T13:13:40.028324Z","shell.execute_reply":"2021-08-23T13:13:40.036751Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"path_to_file = dirPath + 'HalfHeusler.csv'\nfile_name = dirPath + 'Heusler compound.csv'\n\nif (not os.path.exists(file_name)):\n    print (\"A formatted csv file is produced\\n\")\n    df = pd.read_csv(path_to_file)\n    print (df.head())\n\n    id_list = df['Materials-ID'].to_list()\n\n    n = df.columns[0]\n    df.drop(n, axis=1, inplace=True)\n\n    df[n] = [\"mp-\" + str(x) for x in id_list]\n    print (df.head())\n\n    df.to_csv(file_name, index = False)","metadata":{"id":"6414f1e1-9f8a-4548-a684-0d471d7413f6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Lattice parameters, atomic radii and atomic masses. The atomic radius is calculated value and not the empirical values\n- Python library 'pymatgen'","metadata":{"id":"a7ed94a0-6557-417a-b694-03f5d24d26f7"}},{"cell_type":"code","source":"'''\nThe Element class is located in the core subpakage inside the periodic_table module. \nThe link to the API documentation is below.\n\n    https://pymatgen.org/pymatgen.core.periodic_table.html#pymatgen.core.periodic_table.Element\n\nSimilarly the material project APIs are hosted in the following module.\n\n    https://pymatgen.org/pymatgen.ext.matproj.html?highlight=mprester#module-pymatgen.ext.matproj\n'''\n\nfile_name_train = dirPath + 'Training data.csv'\n\nif (not os.path.exists(file_name_train)):\n    print (\"Training data will be generated\\n\")\n\n    heusler_df = pd.read_csv(file_name, header=0, usecols= ['Materials-ID', '4a-site', '4b-site', '4c-site'])\n    data = []\n\n    m = MPRester('fmdc9tZK1xE74JOq')\n    for idx in heusler_df.index:\n        mat_data = m.get_data(heusler_df['Materials-ID'][idx])\n        lat = m.get_structure_by_material_id(heusler_df['Materials-ID'][idx])\n        \n        lat_const = lat.lattice.abc\n        mag_moment = sum(lat.site_properties['magmom'])\n        \n        e1 = pg.Element(heusler_df['4a-site'][idx])\n        e2 = pg.Element(heusler_df['4b-site'][idx])\n        e3 = pg.Element(heusler_df['4c-site'][idx])\n\n        x1 = e1.atomic_radius\n        x2 = e2.atomic_radius\n        x3 = e3.atomic_radius\n        m1 = e1.atomic_mass\n        m2 = e2.atomic_mass\n        m3 = e3.atomic_mass\n        \n        \n        x29 = m1+m2+m3\n        x30 = x1+x2+x3\n        x33 = (x29/3 -m1)\n        x34 = (x29/3 -m2)\n        x35 = (x29/3 -m3)\n        x42 = (x30/3 -x1)\n        x43 = (x30/3 -x2)\n        x44 = (x30/3 -x3)\n        x51 = (x1**2 + x2**2)\n        x52 = (x1**2 + x3**2)\n            \n        data.append((x1,x2,x3,m1,m2,m3,m1**2,m2**2,m3**2,x1**2,x2**2,x3**2,\n                     m1**3,m2**3,m3**3,x1**3,x2**3,x3**3,\n                     sqrt(m1),sqrt(m2),sqrt(m3),sqrt(x1),sqrt(x2),sqrt(x3),\n                     m2/m1, x3/m1, x2/x1,x3/x1, x29,x30,\n                     ((m1**2+m2**2+m3**2)/3.)**2, ((x1**2+x2**2+x3**2)/3.)**2,\n                     x33, x34, x35, abs(x33), abs(x34), abs(x35), x33**2, x34**2, x35**2,\n                     x42, x43, x44, abs(x42), abs(x43), abs(x44), x42**2, x43**2, x44**2,\n                     x51, x52,sqrt(x51), sqrt(x52), \n                     mat_data[0][\"formation_energy_per_atom\"], # mag_moment,\n                     lat_const[0], lat_const[1], lat_const[2]\n                    ))\n\n    idx = []\n    for i in range(1, 55):\n        idx.append('x'+str(i))\n\n#     idx.extend(['form_energy_per_atom', 'total_magnetic_moment','a', 'b', 'c'])\n    idx.extend(['form_energy_per_atom','a', 'b', 'c'])\n    df_train = pd.DataFrame(data, columns = idx)\n\n    df_train.to_csv(file_name_train, index = False)","metadata":{"id":"83bd3313-ef63-4c79-9d93-9bf56c5f4a74"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Machine Learning\nThe gradient boosted trees are selected for the regression task. The cross-validation is the first step.\n","metadata":{"id":"d8b0b11a-42ea-4618-97e2-168a11042613"}},{"cell_type":"markdown","source":"\n### Cross-Validation\nA five fold cross validation will be performed for the better performance of the model.","metadata":{"id":"07770aea-6d95-4202-a1c3-034e20ce9c6f"}},{"cell_type":"code","source":"df = pd.read_csv(dirPath + 'Training data.csv')\nX = df.iloc[:, :-3]\ny = df.iloc[:, -1]\n\nk = 5\nkf = KFold(n_splits=k)\nscoreR2 = []\ny_pred = []\n\ngbReg = GradientBoostingRegressor(loss = 'lad', n_estimators=700, max_depth= 18, random_state= 44)\n\nfor train_idx, test_idx in kf.split(X):\n#     X_train, X_test = X[train_idx, :], X[test_idx, :]\n    X_train, X_test = X.iloc[train_idx, :], X.iloc[test_idx, :]\n    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n    \n    gbReg.fit(X_train, y_train)\n    y_pred.extend(gbReg.predict(X_train))\n    \n    scoreR2.append(gbReg.score(X_test, y_test))\n    \nprint (sum(scoreR2)/k)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ngbReg.fit(X_train, y_train)\nprint (gbReg.score(X_test, y_test))","metadata":{"id":"03900c06-44bc-484f-a04f-fcf4ebf9471e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Grid Search\nHyperparameter tuning","metadata":{"id":"7d0a8b94-3463-4d84-8886-448708fe2b52"}},{"cell_type":"code","source":"# %pip install numpy\n# %pip install matplotlib\n\nmax_depth = [14, 16,18,20,22,24]\nn_estimators = [100, 400, 700, 1000]\nlearning_rate = [0.09, 0.1, 0.11, 0.12]\nparam_grid = dict(max_depth=max_depth, n_estimators= n_estimators)\n\ngrid_search = GridSearchCV(gbReg, param_grid, scoring=\"r2\", n_jobs=-1, cv=kf)\ngrid_result = grid_search.fit(X, y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nprint (\"\\n\")\n\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n\nbest_model = grid_result.best_estimator_\nbest_model.fit(X_train, y_train)\nprint (\"The score for best estimator: \\n\")\nprint (best_model.score(X_test, y_test))\n\nprint (\"\\n\")\n\nfor mean, stdev, param in zip(means, stds, params):\n\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n    \n# plot results\nscores = np.array(means).reshape(len(max_depth), len(n_estimators))\nfor i, value in enumerate(max_depth):\n    plt.plot(n_estimators, scores[i], label='max_depth: ' + str(value))\nplt.legend()\nplt.xlabel('n_estimators')\nplt.ylabel('R2 score')","metadata":{"id":"0c33fafc-61c5-4164-bc6f-7f83a798d671"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = np.array(means).reshape(len(n_estimators),len(max_depth))\nfor i, value in enumerate(n_estimators):\n    plt.plot(max_depth, scores[i], label='n_estimators: ' + str(value))\nplt.legend()\nplt.xlabel('max_depth')\nplt.ylabel('R2 score')\n\n'''\n    lr =0.1\n    max_depth/n_estimators= 16/1000 or 20/700 or 18/700\n'''","metadata":{"id":"NVdRQV2p9TwE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Neural Network \n\nThe Boosted Trees performed very well. It was on par with the published paper. A juxtaposition with an artificial neural network would be interesting. The latter must me a better choice. \n\nANN is easy to build especially after the introduction of Keras module in Tensorflow. \n\n### Scaling\nUse MinMaxScaler","metadata":{"id":"C9isIYEPJ5sy"}},{"cell_type":"code","source":"# The seed is required for replication of results. Parameters defined.\nseed = 1111\nn_cols = 54\n\n# df = pd.read_csv('drive/MyDrive/Training data.csv', usecols= ['x1', 'x2', 'x3','x4', 'x5', 'x6','form_energy_per_atom', 'a', 'b', 'c'])\ndf = pd.read_csv(dirPath + 'Training data.csv')\n\nX = df.iloc[:, :n_cols]\ny = df.iloc[:, -3:]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n\ntrans = MinMaxScaler()\n# trans = StandardScaler()\nX_train_scaled = trans.fit_transform(X_train)\nX_test_scaled = trans.fit_transform(X_test)\nX_scaled = trans.fit_transform(X)\nprint (X_scaled[0,:])\nprint (X_train_scaled[0,:])\nprint (X_test_scaled[0,:])\n","metadata":{"id":"cce1dc4e-9e24-49d1-aff0-2d535265564d","outputId":"4353815a-5b1a-4f73-e12f-30c2f1af75d7","execution":{"iopub.status.busy":"2021-08-23T13:13:46.572193Z","iopub.execute_input":"2021-08-23T13:13:46.572548Z","iopub.status.idle":"2021-08-23T13:13:46.642432Z","shell.execute_reply.started":"2021-08-23T13:13:46.572518Z","shell.execute_reply":"2021-08-23T13:13:46.641449Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[4.78260870e-01 3.91304348e-01 5.60000000e-01 1.64504114e-01\n 5.14650821e-01 9.39921298e-01 3.48502212e-02 3.14376210e-01\n 8.88121115e-01 3.89966555e-01 3.04347826e-01 4.39215686e-01\n 6.71244464e-03 1.82203973e-01 8.37248004e-01 3.09616322e-01\n 2.29194345e-01 3.31960133e-01 3.18154136e-01 6.31446131e-01\n 9.63185617e-01 5.23866112e-01 4.37839988e-01 6.21604719e-01\n 1.42343383e-01 1.08332781e-01 5.11347765e-01 4.79681559e-01\n 5.90394941e-01 4.71698113e-01 2.38831667e-01 2.50094473e-01\n 9.97778804e-01 5.23512429e-01 2.39481808e-01 7.55739640e-01\n 1.35823689e-02 5.93217710e-01 5.71444605e-01 2.04636267e-04\n 3.54534887e-01 4.72222222e-01 4.88372093e-01 3.78378378e-01\n 3.20000000e-01 4.34782609e-02 2.33333333e-01 1.02400000e-01\n 1.89035917e-03 5.44444444e-02 2.93858036e-01 4.57718121e-01\n 3.64911871e-01 5.41324956e-01]\n[0.7826087  0.43478261 1.         0.57105744 0.         0.97680217\n 0.33998752 0.         0.95637163 0.72240803 0.34506556 1.\n 0.19871264 0.         0.93543885 0.6593259  0.26570825 1.\n 0.71529774 0.         0.98568378 0.81067122 0.48198743 1.\n 0.         0.03561061 0.31155995 0.56539047 0.60345077 0.83018868\n 0.30116907 0.66971477 0.47476816 1.         0.21607835 0.16379484\n 0.93719638 0.63297068 0.02705297 0.87897278 0.4031816  0.41666667\n 0.86046512 0.         0.4        0.73913043 0.24137931 0.16\n 0.5463138  0.05826397 0.49834498 1.         0.57334451 1.        ]\n[0.5        0.38888889 0.875      0.66775195 0.5310495  0.26432593\n 0.4587802  0.33137576 0.0868844  0.44444444 0.31989247 0.83333333\n 0.31122076 0.19671734 0.02636676 0.39068826 0.25755361 0.7879416\n 0.78625077 0.64584152 0.42231172 0.52804098 0.4251467  0.89379021\n 0.0370635  0.01362502 0.18032311 0.546875   0.51816704 0.52272727\n 0.15218231 0.32167054 0.12897023 0.48244108 0.82005331 0.73214023\n 0.10218001 0.59233334 0.55593231 0.01060271 0.35598528 0.20689655\n 0.67857143 0.38709677 0.68421053 0.35714286 0.36666667 0.52\n 0.12755102 0.13444444 0.33094213 0.57441992 0.38294233 0.63200555]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## The parameters\n\n#### 6,4500-5500 best so far for epoch 1800-2000\n\n(2, 600)/({nl-3, nn-800/nn-600, ep-2000}: 0.89)\n\n({nl-5, nn-3200, ep-1700}: 0.87)\n\n({nl-5, nn-3500, ep-1800}: 0.906)\n\n5, 2500-3500 epochs 500\n\n7, 2500 by GridSearchCV in kaggle\n3, 1200 for one column output for epoch 850","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import backend as K\n\n# tolerance_value = 90\nn_epochs = 1600\n\n# coefficient of determination (R^2) for regression  (only for Keras tensors)\ndef r_square(y_true, y_pred):\n    SS_res =  K.sum(K.square(y_true - y_pred)) \n    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n    return ( 1 - SS_res/SS_tot + K.epsilon())\n\ndef build_model(n_hidden=6, n_neurons=4500, learning_rate=1e-4, input_shape=[n_cols,]):\n    model = keras.Sequential()\n    model.add(keras.layers.InputLayer(input_shape=input_shape))\n    for layer in range(n_hidden):\n        model.add(keras.layers.Dense(n_neurons, activation=\"sigmoid\"))\n    model.add(keras.layers.Dense(3))\n\n    model.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam(lr=learning_rate), metrics=[r_square])\n\n    return model\n\n# The wrapper is neccessary for GridSearch later in this notebook\nkeras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n\n# The history dictionary is useful for 'loss' plot of the model. In this case, the 'loss' is mean absolute error \nhistory = keras_reg.fit(X_train_scaled, y_train, epochs=n_epochs,\n              validation_data=(X_test_scaled, y_test))\n            #   callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=tolerance_value)])\n\n# Test on the holdout data\nmse_test = keras_reg.score(X_test_scaled, y_test)\ny_pred = keras_reg.predict(X_test_scaled)\n\nplt.figure()\nplt.plot(history.history['r_square'])\nplt.plot(history.history['val_r_square'])\nplt.ylim(0.85,1)\nplt.xlim(250)\nplt.title(\"Model co-efficient of determination\")\nplt.ylabel(\"R-squared score\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"Train\", \"Test\"])\nplt.show()\n\n# print (\"\\n The predicted values (Lattice constants)\")\n# print (y_pred)\nprint (\"\\n The mean square error (MSE)\")\nprint (mse_test)\nprint (\"\\n The R-square metric is\")\nprint (r_square(tf.convert_to_tensor(value=y_test.values, dtype='float32'), y_pred).numpy())","metadata":{"id":"0df9mMKWKL-A","outputId":"ac4173bf-ac36-49ae-9b52-e1fcad59a5df","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Grid Search\n\nThe hyperparamter tuning for NN.","metadata":{"id":"rH1XkCwTvSIo"}},{"cell_type":"code","source":"# tolerance_value = 50\nfrom sklearn.metrics import make_scorer\n\n# coefficient of determination (R^2) for regression  (only for Keras tensors)\ndef r_square(y_true, y_pred):\n#     y_true = tf.convert_to_tensor(value=y_true, dtype='float32')\n   \n    SS_res =  K.sum(K.square(y_true - y_pred)) \n    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n    return (1 - SS_res/(SS_tot + K.epsilon())).numpy()\n\nparam_distribs = {\n#     \"n_hidden\": [4, 5, 6, 7, 8],\n#     \"n_neurons\": [1500, 2000, 2500, 3000, 3500, 4000],\n    \"n_hidden\": [5],\n    \"n_neurons\": [2500, 3000, 3500, 4000],\n}\n\nsearch_cv = GridSearchCV(estimator= keras_reg,\n                         param_grid= param_distribs, \n                         cv= 4,\n                         scoring =make_scorer(r_square))\n\ngrid_result = search_cv.fit(X_scaled, y.to_numpy())\n#                 validation_data=(X_test_scaled, y_test),\n                # callbacks=[keras.callbacks.EarlyStopping(monitor='loss', patience=tolerance_value)])\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"mean: %f, std: (%f) parameters: %r\" % (mean, stdev, param))","metadata":{"id":"ThABmFyXvXYu","outputId":"50ba5cd1-c957-4acd-d792-aff660960ab3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model = grid_result.best_estimator_\n\nprint (\"\\nHistory object\\n\")\nbest_model_history = best_model.fit(X_train_scaled, y_train, epochs=n_epochs,\n              validation_data=(X_test_scaled, y_test),\n              callbacks=[keras.callbacks.EarlyStopping(patience=tolerance_value)])\n\ny_pred = best_model.predict(X_test)              \n\nplt.figure()\nplt.plot(best_model_history.history['loss'])\nplt.plot(best_model_history.history['val_loss'])\nplt.title(\"Model loss\")\nplt.ylabel(\"Mean Square Error (MSE) - Loss\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"Train\", \"Test\"])\nplt.show()\n\nprint (\"\\n Best Model's Parameters\")\nprint (best_model.get_params())\nprint (\"\\n The mean square error (MSE)\")\nprint (best_model.score(X_test, y_test))\nprint (\"\\n The R-square metric is\")\nprint (r_square(tf.convert_to_tensor(value=y_test.values, dtype='float32'), y_pred))\nprint (\"\\n The predicted values (Lattice constants)\")\nprint (y_pred)","metadata":{"id":"8UoimM0Ev7je"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nplt.plot(history.history['r_square'])\nplt.plot(history.history['val_r_square'])\nplt.ylim(0.9,1)\nplt.xlim(250)\nplt.title(\"Model co-efficient of determination\")\nplt.ylabel(\"R-squared score\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"Train\", \"Test\"])\nplt.show()\n","metadata":{"id":"ruRDKmyblOSU","outputId":"d595b0a2-914f-44f1-b4ec-f806f8583344","execution":{"iopub.status.busy":"2021-08-23T13:39:24.014704Z","iopub.execute_input":"2021-08-23T13:39:24.015327Z","iopub.status.idle":"2021-08-23T13:39:24.092049Z","shell.execute_reply.started":"2021-08-23T13:39:24.015198Z","shell.execute_reply":"2021-08-23T13:39:24.090407Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-6e3e635f3915>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'r_square'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_r_square'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"],"ename":"NameError","evalue":"name 'plt' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"id":"GgxAS5cYlWsP"},"execution_count":null,"outputs":[]}]}