{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76e9a234-e8d2-4c06-bc62-3fcfaba7b642",
   "metadata": {
    "id": "76e9a234-e8d2-4c06-bc62-3fcfaba7b642"
   },
   "source": [
    "## Introduction\n",
    "Where did data come from?\n",
    "\n",
    "The material id needs to be prepended by \"mp-\". The job is performed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zwdUed_ccjQt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zwdUed_ccjQt",
    "outputId": "8d9ae26e-79fe-4405-a901-a37c52f04292"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ff8be58-9ef0-4180-97fe-af8210c01cff",
   "metadata": {
    "id": "1ff8be58-9ef0-4180-97fe-af8210c01cff"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# module_path = os.path.abspath(os.path.join(''))\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "    \n",
    "# print (module_path)\n",
    "\n",
    "# from utils.editdf import EditFile \n",
    "\n",
    "# ed = EditFile()\n",
    "# ed.generateFile('assets/HalfHeusler.csv', 'assets/Heusler compound.csv')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# %pip install pymatgen\n",
    "# %pip install sklearn\n",
    "\n",
    "import os.path\n",
    "import pandas as pd\n",
    "\n",
    "# import pymatgen.core as pg\n",
    "# from pymatgen.ext.matproj import MPRester\n",
    "from math import sqrt\n",
    "import sys\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# dirPath= 'drive/Mydrive/'\n",
    "dirPath= 'assets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6414f1e1-9f8a-4548-a684-0d471d7413f6",
   "metadata": {
    "id": "6414f1e1-9f8a-4548-a684-0d471d7413f6"
   },
   "outputs": [],
   "source": [
    "path_to_file = dirPath + 'HalfHeusler.csv'\n",
    "file_name = dirPath + 'Heusler compound.csv'\n",
    "\n",
    "if (not os.path.exists(file_name)):\n",
    "    print (\"A formatted csv file is produced\\n\")\n",
    "    df = pd.read_csv(path_to_file)\n",
    "    print (df.head())\n",
    "\n",
    "    id_list = df['Materials-ID'].to_list()\n",
    "\n",
    "    n = df.columns[0]\n",
    "    df.drop(n, axis=1, inplace=True)\n",
    "\n",
    "    df[n] = [\"mp-\" + str(x) for x in id_list]\n",
    "    print (df.head())\n",
    "\n",
    "    df.to_csv(file_name, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ed94a0-6557-417a-b694-03f5d24d26f7",
   "metadata": {
    "id": "a7ed94a0-6557-417a-b694-03f5d24d26f7"
   },
   "source": [
    "- Lattice parameters, atomic radii and atomic masses. The atomic radius is calculated value and not the empirical values\n",
    "- Python library 'pymatgen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bd3313-ef63-4c79-9d93-9bf56c5f4a74",
   "metadata": {
    "id": "83bd3313-ef63-4c79-9d93-9bf56c5f4a74"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The Element class is located in the core subpakage inside the periodic_table module. \n",
    "The link to the API documentation is below.\n",
    "\n",
    "    https://pymatgen.org/pymatgen.core.periodic_table.html#pymatgen.core.periodic_table.Element\n",
    "\n",
    "Similarly the material project APIs are hosted in the following module.\n",
    "\n",
    "    https://pymatgen.org/pymatgen.ext.matproj.html?highlight=mprester#module-pymatgen.ext.matproj\n",
    "'''\n",
    "\n",
    "file_name_train = dirPath + 'Training data.csv'\n",
    "\n",
    "if (not os.path.exists(file_name_train)):\n",
    "    print (\"Training data will be generated\\n\")\n",
    "\n",
    "    heusler_df = pd.read_csv(file_name, header=0, usecols= ['Materials-ID', '4a-site', '4b-site', '4c-site'])\n",
    "    data = []\n",
    "\n",
    "    m = MPRester('fmdc9tZK1xE74JOq')\n",
    "    for idx in heusler_df.index:\n",
    "        mat_data = m.get_data(heusler_df['Materials-ID'][idx])\n",
    "        lat = m.get_structure_by_material_id(heusler_df['Materials-ID'][idx])\n",
    "        \n",
    "        lat_const = lat.lattice.abc\n",
    "        mag_moment = sum(lat.site_properties['magmom'])\n",
    "        \n",
    "        e1 = pg.Element(heusler_df['4a-site'][idx])\n",
    "        e2 = pg.Element(heusler_df['4b-site'][idx])\n",
    "        e3 = pg.Element(heusler_df['4c-site'][idx])\n",
    "\n",
    "        x1 = e1.atomic_radius\n",
    "        x2 = e2.atomic_radius\n",
    "        x3 = e3.atomic_radius\n",
    "        m1 = e1.atomic_mass\n",
    "        m2 = e2.atomic_mass\n",
    "        m3 = e3.atomic_mass\n",
    "        \n",
    "        \n",
    "        x29 = m1+m2+m3\n",
    "        x30 = x1+x2+x3\n",
    "        x33 = (x29/3 -m1)\n",
    "        x34 = (x29/3 -m2)\n",
    "        x35 = (x29/3 -m3)\n",
    "        x42 = (x30/3 -x1)\n",
    "        x43 = (x30/3 -x2)\n",
    "        x44 = (x30/3 -x3)\n",
    "        x51 = (x1**2 + x2**2)\n",
    "        x52 = (x1**2 + x3**2)\n",
    "            \n",
    "        data.append((x1,x2,x3,m1,m2,m3,m1**2,m2**2,m3**2,x1**2,x2**2,x3**2,\n",
    "                     m1**3,m2**3,m3**3,x1**3,x2**3,x3**3,\n",
    "                     sqrt(m1),sqrt(m2),sqrt(m3),sqrt(x1),sqrt(x2),sqrt(x3),\n",
    "                     m2/m1, x3/m1, x2/x1,x3/x1, x29,x30,\n",
    "                     ((m1**2+m2**2+m3**2)/3.)**2, ((x1**2+x2**2+x3**2)/3.)**2,\n",
    "                     x33, x34, x35, abs(x33), abs(x34), abs(x35), x33**2, x34**2, x35**2,\n",
    "                     x42, x43, x44, abs(x42), abs(x43), abs(x44), x42**2, x43**2, x44**2,\n",
    "                     x51, x52,sqrt(x51), sqrt(x52), \n",
    "                     mat_data[0][\"formation_energy_per_atom\"], # mag_moment,\n",
    "                     lat_const[0], lat_const[1], lat_const[2]\n",
    "                    ))\n",
    "\n",
    "    idx = []\n",
    "    for i in range(1, 55):\n",
    "        idx.append('x'+str(i))\n",
    "\n",
    "#     idx.extend(['form_energy_per_atom', 'total_magnetic_moment','a', 'b', 'c'])\n",
    "    idx.extend(['form_energy_per_atom','a', 'b', 'c'])\n",
    "    df_train = pd.DataFrame(data, columns = idx)\n",
    "\n",
    "    df_train.to_csv(file_name_train, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b0b11a-42ea-4618-97e2-168a11042613",
   "metadata": {
    "id": "d8b0b11a-42ea-4618-97e2-168a11042613"
   },
   "source": [
    "## Machine Learning\n",
    "The gradient boosted trees are selected for the regression task. The cross-validation is the first step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07770aea-6d95-4202-a1c3-034e20ce9c6f",
   "metadata": {
    "id": "07770aea-6d95-4202-a1c3-034e20ce9c6f"
   },
   "source": [
    "\n",
    "### Cross-Validation\n",
    "A five fold cross validation will be performed for the better performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03900c06-44bc-484f-a04f-fcf4ebf9471e",
   "metadata": {
    "id": "03900c06-44bc-484f-a04f-fcf4ebf9471e"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(dirPath + 'Training data.csv')\n",
    "X = df.iloc[:, :-3]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "k = 5\n",
    "kf = KFold(n_splits=k)\n",
    "scoreR2 = []\n",
    "y_pred = []\n",
    "\n",
    "gbReg = GradientBoostingRegressor(loss = 'lad', n_estimators=700, max_depth= 18, random_state= 44)\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "#     X_train, X_test = X[train_idx, :], X[test_idx, :]\n",
    "    X_train, X_test = X.iloc[train_idx, :], X.iloc[test_idx, :]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    gbReg.fit(X_train, y_train)\n",
    "    y_pred.extend(gbReg.predict(X_train))\n",
    "    \n",
    "    scoreR2.append(gbReg.score(X_test, y_test))\n",
    "    \n",
    "print (sum(scoreR2)/k)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "gbReg.fit(X_train, y_train)\n",
    "print (gbReg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0a8b94-3463-4d84-8886-448708fe2b52",
   "metadata": {
    "id": "7d0a8b94-3463-4d84-8886-448708fe2b52"
   },
   "source": [
    "## Grid Search\n",
    "Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c33fafc-61c5-4164-bc6f-7f83a798d671",
   "metadata": {
    "id": "0c33fafc-61c5-4164-bc6f-7f83a798d671"
   },
   "outputs": [],
   "source": [
    "# %pip install numpy\n",
    "# %pip install matplotlib\n",
    "\n",
    "max_depth = [14, 16,18,20,22,24]\n",
    "n_estimators = [100, 400, 700, 1000]\n",
    "learning_rate = [0.09, 0.1, 0.11, 0.12]\n",
    "param_grid = dict(max_depth=max_depth, n_estimators= n_estimators)\n",
    "\n",
    "grid_search = GridSearchCV(gbReg, param_grid, scoring=\"r2\", n_jobs=-1, cv=kf)\n",
    "grid_result = grid_search.fit(X, y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "print (\"\\n\")\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "best_model = grid_result.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "print (\"The score for best estimator: \\n\")\n",
    "print (best_model.score(X_test, y_test))\n",
    "\n",
    "print (\"\\n\")\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    \n",
    "# plot results\n",
    "scores = np.array(means).reshape(len(max_depth), len(n_estimators))\n",
    "for i, value in enumerate(max_depth):\n",
    "    plt.plot(n_estimators, scores[i], label='max_depth: ' + str(value))\n",
    "plt.legend()\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('R2 score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NVdRQV2p9TwE",
   "metadata": {
    "id": "NVdRQV2p9TwE"
   },
   "outputs": [],
   "source": [
    "scores = np.array(means).reshape(len(n_estimators),len(max_depth))\n",
    "for i, value in enumerate(n_estimators):\n",
    "    plt.plot(max_depth, scores[i], label='n_estimators: ' + str(value))\n",
    "plt.legend()\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('R2 score')\n",
    "\n",
    "'''\n",
    "    lr =0.1\n",
    "    max_depth/n_estimators= 16/1000 or 20/700 or 18/700\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C9isIYEPJ5sy",
   "metadata": {
    "id": "C9isIYEPJ5sy"
   },
   "source": [
    "## Neural Network \n",
    "\n",
    "The Boosted Trees performed very well. It was on par with the published paper. A juxtaposition with an artificial neural network would be interesting. The latter must me a better choice. \n",
    "\n",
    "ANN is easy to build especially after the introduction of Keras module in Tensorflow. \n",
    "\n",
    "### Scaling\n",
    "Use MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce1dc4e-9e24-49d1-aff0-2d535265564d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cce1dc4e-9e24-49d1-aff0-2d535265564d",
    "outputId": "4353815a-5b1a-4f73-e12f-30c2f1af75d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               x1          x2          x3          x4          x5          x6  \\\n",
      "count  143.000000  143.000000  143.000000  143.000000  143.000000  143.000000   \n",
      "mean     1.638462    1.489860    1.331119   98.842454  124.909457  114.067291   \n",
      "std      0.190042    0.175784    0.132527   66.694788   59.919883   61.227592   \n",
      "min      1.050000    1.000000    0.650000    6.941000   22.989769    9.012182   \n",
      "25%      1.450000    1.450000    1.350000   44.955912  107.868200   58.693400   \n",
      "50%      1.750000    1.450000    1.350000   91.224000  121.760000  106.420000   \n",
      "75%      1.750000    1.600000    1.400000  164.930320  207.200000  195.084000   \n",
      "max      2.200000    2.150000    1.900000  238.028910  208.980400  208.980400   \n",
      "\n",
      "                 x7            x8            x9         x10  ...         x49  \\\n",
      "count    143.000000    143.000000    143.000000  143.000000  ...  143.000000   \n",
      "mean   14186.919206  19167.657158  16733.949381    2.720420  ...    0.012154   \n",
      "std    13874.279555  15441.154598  15255.990554    0.623035  ...    0.021966   \n",
      "min       48.177481    528.529492     81.219424    1.102500  ...    0.000000   \n",
      "25%     2021.034024  11635.548571   3444.915204    2.102500  ...    0.001111   \n",
      "50%     8321.818176  14825.497600  11325.216400    3.062500  ...    0.004444   \n",
      "75%    27202.010455  42931.840000  38057.767056    3.062500  ...    0.008472   \n",
      "max    56657.761996  43672.807584  43672.807584    4.840000  ...    0.146944   \n",
      "\n",
      "                x50         x51         x52         x53         x54  \\\n",
      "count  1.430000e+02  143.000000  143.000000  143.000000  143.000000   \n",
      "mean   3.718920e-02    4.970787    4.509738    2.218464    2.115104   \n",
      "std    4.268110e-02    1.009069    0.796362    0.222604    0.190595   \n",
      "min    4.930381e-32    2.665000    1.825000    1.632483    1.350926   \n",
      "25%    8.472222e-03    4.296250    3.925000    2.072695    1.981161   \n",
      "50%    2.777778e-02    5.165000    4.885000    2.272664    2.210204   \n",
      "75%    4.694444e-02    5.622500    5.022500    2.371181    2.241093   \n",
      "max    2.500000e-01    9.462500    7.412500    3.076118    2.722591   \n",
      "\n",
      "       form_energy_per_atom           a           b           c  \n",
      "count            143.000000  143.000000  143.000000  143.000000  \n",
      "mean              -0.704601    4.540059    4.540059    4.540059  \n",
      "std                0.311848    0.314252    0.314252    0.314252  \n",
      "min               -1.282131    3.490060    3.490060    3.490060  \n",
      "25%               -0.946281    4.350484    4.350484    4.350484  \n",
      "50%               -0.693089    4.560883    4.560883    4.560883  \n",
      "75%               -0.500694    4.733690    4.733690    4.733690  \n",
      "max                0.167805    5.610072    5.610071    5.610072  \n",
      "\n",
      "[8 rows x 58 columns]\n",
      "     x1    x2    x3          x4          x5          x6            x7  \\\n",
      "0  1.60  1.45  1.35   44.955912  118.710000  196.966569   2021.034024   \n",
      "1  1.45  1.25  1.10    6.941000   26.981539   28.085500     48.177481   \n",
      "2  1.60  1.45  1.35   44.955912  121.760000   58.693400   2021.034024   \n",
      "3  1.75  1.45  1.35  168.934210  118.710000  196.966569  28538.767308   \n",
      "4  1.50  1.45  1.35   24.305000  121.760000   63.546000    590.733025   \n",
      "\n",
      "             x8            x9     x10  ...       x45       x46       x47  \\\n",
      "0  14092.064100  38795.829304  2.5600  ...  0.133333  0.016667  0.116667   \n",
      "1    728.003425    788.795310  2.1025  ...  0.183333  0.016667  0.166667   \n",
      "2  14825.497600   3444.915204  2.5600  ...  0.133333  0.016667  0.116667   \n",
      "3  14092.064100  38795.829304  3.0625  ...  0.233333  0.066667  0.166667   \n",
      "4  14825.497600   4038.094116  2.2500  ...  0.066667  0.016667  0.083333   \n",
      "\n",
      "        x48       x49       x50     x51     x52       x53       x54  \n",
      "0  0.017778  0.000278  0.013611  4.6625  4.3825  2.159282  2.093442  \n",
      "1  0.033611  0.000278  0.027778  3.6650  3.3125  1.914419  1.820027  \n",
      "2  0.017778  0.000278  0.013611  4.6625  4.3825  2.159282  2.093442  \n",
      "3  0.054444  0.004444  0.027778  5.1650  4.8850  2.272664  2.210204  \n",
      "4  0.004444  0.000278  0.006944  4.3525  4.0725  2.086265  2.018044  \n",
      "\n",
      "[5 rows x 54 columns]\n",
      "          a         b         c\n",
      "0  4.611608  4.611608  4.611608\n",
      "1  4.199121  4.199121  4.199121\n",
      "2  4.320950  4.320950  4.320950\n",
      "3  4.726488  4.726488  4.726488\n",
      "4  4.412778  4.412778  4.412778\n"
     ]
    }
   ],
   "source": [
    "# The seed is required for replication of results. Parameters defined.\n",
    "seed = 1111\n",
    "n_cols = 54\n",
    "\n",
    "# df = pd.read_csv('drive/MyDrive/Training data.csv', usecols= ['x1', 'x2', 'x3','x4', 'x5', 'x6','form_energy_per_atom', 'a', 'b', 'c'])\n",
    "df = pd.read_csv(dirPath + 'Training data.csv')\n",
    "\n",
    "print (df.describe())\n",
    "\n",
    "X = df.iloc[:, :n_cols]\n",
    "y = df.iloc[:, -3:]\n",
    "\n",
    "print  (X.head())\n",
    "print (y.head())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "trans = MinMaxScaler()\n",
    "# trans = StandardScaler()\n",
    "X_train_scaled = trans.fit_transform(X_train)\n",
    "X_test_scaled = trans.fit_transform(X_test)\n",
    "X_scaled = trans.fit_transform(X)\n",
    "# print (X_scaled[0,:])\n",
    "# print (X_train_scaled[0,:])\n",
    "# print (X_test_scaled[0,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df9mMKWKL-A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0df9mMKWKL-A",
    "outputId": "ac4173bf-ac36-49ae-9b52-e1fcad59a5df"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# tolerance_value = 90\n",
    "n_epochs = 1000\n",
    "\n",
    "# coefficient of determination (R^2) for regression  (only for Keras tensors)\n",
    "def r_square(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return (1 - SS_res/SS_tot + K.epsilon())\n",
    "\n",
    "# def det_coeff(y_true, y_pred):\n",
    "#     u = K.sum(K.square(y_true - y_pred))\n",
    "#     v = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "#     return K.ones_like(v) - (u / v)\n",
    "\n",
    "# 6,2750-5500 best so far for epoch 600-1000\n",
    "\n",
    "# 5, 2400 epochs 500\n",
    "# 7, 2500 by GridSearchCV in kaggle\n",
    "# 3, 1200 for one column output for epoch 850\n",
    "def build_model(n_hidden=6, n_neurons=2750, learning_rate=1e-4, input_shape=[n_cols,]):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"sigmoid\"))\n",
    "    model.add(keras.layers.Dense(3))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam(lr=learning_rate), metrics=[r_square])\n",
    "\n",
    "    return model\n",
    "\n",
    "# The wrapper is neccessary for GridSearch later in this notebook\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
    "\n",
    "# The history dictionary is useful for 'loss' plot of the model. In this case, the 'loss' is mean absolute error \n",
    "history = keras_reg.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "              validation_data=(X_test_scaled, y_test))\n",
    "            #   callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=tolerance_value)])\n",
    "\n",
    "# Test on the holdout data\n",
    "mse_test = keras_reg.score(X_test_scaled, y_test)\n",
    "y_pred = keras_reg.predict(X_test_scaled)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['r_square'])\n",
    "plt.plot(history.history['val_r_square'])\n",
    "plt.ylim(0,1)\n",
    "plt.title(\"Model co-efficient of determination\")\n",
    "plt.ylabel(\"R-squared score\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Test\"])\n",
    "plt.show()\n",
    "\n",
    "# print (\"\\n The predicted values (Lattice constants)\")\n",
    "# print (y_pred)\n",
    "print (\"\\n The mean square error (MSE)\")\n",
    "print (mse_test)\n",
    "print (\"\\n The R-square metric is\")\n",
    "print (r_square(tf.convert_to_tensor(value=y_test.values, dtype='float32'), y_pred).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rH1XkCwTvSIo",
   "metadata": {
    "id": "rH1XkCwTvSIo"
   },
   "source": [
    "## Grid Search\n",
    "\n",
    "The hyperparamter tuning for NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ThABmFyXvXYu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "ThABmFyXvXYu",
    "outputId": "50ba5cd1-c957-4acd-d792-aff660960ab3"
   },
   "outputs": [],
   "source": [
    "# tolerance_value = 50\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [4, 5, 6, 7, 8],\n",
    "    \"n_neurons\": [1500, 2000, 2500, 3000, 3500, 4000],\n",
    "}\n",
    "\n",
    "search_cv = GridSearchCV(estimator= keras_reg,\n",
    "                         param_grid= param_distribs, \n",
    "                         cv= 4, \n",
    "                         scoring ={'coeff_determination': make_scorer(r_square)})\n",
    "\n",
    "# Best estimator is selected for further proceedings\n",
    "grid_result = search_cv.fit(X_scaled, y)\n",
    "                # validation_data=(X_test_scaled, y_test),\n",
    "                # callbacks=[keras.callbacks.EarlyStopping(monitor='loss', patience=tolerance_value)])\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"mean: %f, std: (%f) parameters: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8UoimM0Ev7je",
   "metadata": {
    "id": "8UoimM0Ev7je"
   },
   "outputs": [],
   "source": [
    "best_model = grid_result.best_estimator_\n",
    "\n",
    "print (\"\\nHistory object\\n\")\n",
    "best_model_history = best_model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "              validation_data=(X_test_scaled, y_test),\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=tolerance_value)])\n",
    "\n",
    "y_pred = best_model.predict(X_test)              \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(best_model_history.history['loss'])\n",
    "plt.plot(best_model_history.history['val_loss'])\n",
    "plt.title(\"Model loss\")\n",
    "plt.ylabel(\"Mean Square Error (MSE) - Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Test\"])\n",
    "plt.show()\n",
    "\n",
    "print (\"\\n Best Model's Parameters\")\n",
    "print (best_model.get_params())\n",
    "print (\"\\n The mean square error (MSE)\")\n",
    "print (best_model.score(X_test, y_test))\n",
    "print (\"\\n The R-square metric is\")\n",
    "print (r_square(tf.convert_to_tensor(value=y_test.values, dtype='float32'), y_pred))\n",
    "print (\"\\n The predicted values (Lattice constants)\")\n",
    "print (y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruRDKmyblOSU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ruRDKmyblOSU",
    "outputId": "d595b0a2-914f-44f1-b4ec-f806f8583344"
   },
   "outputs": [],
   "source": [
    "\n",
    "def r_square(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    print (K.square(y_true - y_pred))\n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return ( 1 - SS_res/SS_tot + K.epsilon())\n",
    "\n",
    "print (r_square(tf.convert_to_tensor(value=y_test.values, dtype='float32'), y_pred).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GgxAS5cYlWsP",
   "metadata": {
    "id": "GgxAS5cYlWsP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "main-HH.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
